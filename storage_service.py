# ==============================================================================
# storage_service.py
# ------------------------------------------------------------------------------
# ROL: GestioneazƒÉ stocare fi»ôiere √Æn S3-Compatible Storage (ex: Scaleway, R2, AWS)
#      ImplementeazƒÉ upload/download/delete pentru CSV, PDF, PNG
#
# ARHITECTURƒÇ:
#   - S3 Storage: Storage persistent cloud
#   - boto3: Client Python pentru opera»õii S3-compatible
#   - Fallback local: DacƒÉ S3 nu e disponibil, salveazƒÉ local
#
# RESPECTƒÇ: .cursorrules - Privacy by Design (zero date personale!)
# ==============================================================================

import os
import io
import boto3
from botocore.client import Config
from botocore.exceptions import ClientError, BotoCoreError
from typing import Optional, BinaryIO, Union
from logger_setup import logger

# --- Configurare S3 din Environment Variables ---
# NOTƒÇ: SuportƒÉ 3 naming conventions:
#   1. S3_* (generic, prioritate 1)
#   2. R2_* (Cloudflare R2 legacy, prioritate 2)
#   3. SCW_* (Scaleway Object Storage, prioritate 3)

# Helper: DetectƒÉm dacƒÉ folosim Scaleway
SCW_ACCESS_KEY = os.getenv('SCW_ACCESS_KEY', '')
SCW_SECRET_KEY = os.getenv('SCW_SECRET_KEY', '')
SCW_REGION = os.getenv('SCW_DEFAULT_REGION', os.getenv('SCW_REGION', 'fr-par'))  # Default: Paris
SCW_BUCKET = os.getenv('SCW_BUCKET_NAME', 'pulsoximetrie')

# DacƒÉ avem variabile SCW, construim endpoint-ul Scaleway automat
if SCW_ACCESS_KEY and SCW_SECRET_KEY:
    SCW_ENDPOINT = f"https://s3.{SCW_REGION}.scw.cloud"
    logger.warning(f"üîç [SCALEWAY_DETECTED] Auto-constructing endpoint: {SCW_ENDPOINT}")
else:
    SCW_ENDPOINT = ''

# Fallback chain: S3_* ‚Üí R2_* ‚Üí SCW_*
S3_ENABLED = os.getenv('S3_ENABLED', os.getenv('R2_ENABLED', 'True' if SCW_ACCESS_KEY else 'False')).lower() == 'true'
S3_ENDPOINT = os.getenv('S3_ENDPOINT', os.getenv('R2_ENDPOINT', SCW_ENDPOINT))
S3_ACCESS_KEY_ID = os.getenv('S3_ACCESS_KEY_ID', os.getenv('R2_ACCESS_KEY_ID', SCW_ACCESS_KEY))
S3_SECRET_ACCESS_KEY = os.getenv('S3_SECRET_ACCESS_KEY', os.getenv('R2_SECRET_ACCESS_KEY', SCW_SECRET_KEY))
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME', os.getenv('R2_BUCKET_NAME', SCW_BUCKET))
S3_REGION = os.getenv('S3_REGION', os.getenv('R2_REGION', SCW_REGION))

# Fallback pentru stocare localƒÉ
LOCAL_STORAGE_DIR = "patient_data"


# ==============================================================================
# S3 GENERIC CLIENT - S3-COMPATIBLE
# ==============================================================================

class S3StorageClient:
    """
    Client generic pentru interac»õiune cu orice S3-compatible storage (Scaleway, R2, AWS).
    
    Features:
    - Upload fi»ôiere (CSV, PDF, PNG)
    - Download fi»ôiere (stream sau bytes)
    - Delete fi»ôiere
    - List fi»ôiere din folder
    - Generate signed URLs (op»õional)
    """
    
    def __init__(self):
        """Ini»õializeazƒÉ client-ul S3 cu creden»õiale din environment."""
        self.enabled = S3_ENABLED
        self.bucket_name = S3_BUCKET_NAME
        self.client = None
        self.init_error = None # [DIAGNOSTIC] Capture exact error
        
        if not self.enabled:
            self.init_error = "S3_ENABLED env var is False/Missing"
            logger.warning("‚ö†Ô∏è S3 Storage DEZACTIVAT - folosim stocare LOCALƒÇ")
            return
        
        if not all([S3_ENDPOINT, S3_ACCESS_KEY_ID, S3_SECRET_ACCESS_KEY]):
            self.init_error = "Missing Credentials (ENDPOINT/KEY/SECRET)"
            logger.error("‚ùå Creden»õiale S3 incomplete! SeteazƒÉ S3_ENDPOINT, S3_ACCESS_KEY_ID, S3_SECRET_ACCESS_KEY")
            self.enabled = False
            return
        
        try:
            # Ini»õializare client boto3 pentru S3
            self.client = boto3.client(
                's3',
                endpoint_url=S3_ENDPOINT,
                aws_access_key_id=S3_ACCESS_KEY_ID,
                aws_secret_access_key=S3_SECRET_ACCESS_KEY,
                region_name=S3_REGION,
                config=Config(signature_version='s3v4')
            )
            
            # [ITERATION 5] Test conexiune - try HEAD, but don't fail if 403
            try:
                self.client.head_bucket(Bucket=self.bucket_name)
                logger.warning(f"‚úÖ [S3_TRACE_INIT] S3 Storage conectat cu succes! (Read Access OK)")
            except ClientError as head_err:
                error_code = head_err.response.get('Error', {}).get('Code', 'Unknown')
                logger.warning(f"‚ö†Ô∏è [S3_TRACE_INIT] head_bucket FAILED: {error_code}")
                if error_code == '403':
                    logger.warning(f"‚ö†Ô∏è [S3_READ_PERM] Token lacks READ permission (head_bucket denied)")
                    logger.warning(f"‚ö†Ô∏è [S3_READ_PERM] Will still attempt WRITE test...")
                elif error_code == '404':
                    logger.critical(f"‚ùå [S3_BUCKET] Bucket '{self.bucket_name}' NOT FOUND!")
                    self.init_error = f"Bucket '{self.bucket_name}' not found"
                    self.enabled = False
                    return
                # Don't disable S3 yet - maybe write works even if read doesn't
            
            logger.warning(f"   - Endpoint: {S3_ENDPOINT}")
            logger.warning(f"   - Bucket: {self.bucket_name}")
            logger.warning(f"   - Region: {S3_REGION}")
            
            # [ITERATION 5] Check Write Permissions ALWAYS (even if head failed)
            self._check_write_permission()
            
            # [ITERATION 2] Log boto3 client configuration details
            logger.warning(f"üîç [S3_CONFIG] Signature Version: s3v4")
            logger.warning(f"üîç [S3_CONFIG] Using boto3 client with endpoint: {S3_ENDPOINT}")
            
            # If we got here without errors, consider S3 enabled
            if self.init_error is None:
                self.init_error = None  # Explicitly clear
            
        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', 'Unknown')
            if error_code == '404':
                msg = f"Bucket '{self.bucket_name}' NOT FOUND (404)"
            elif error_code == '403':
                msg = f"Access DENIED to bucket '{self.bucket_name}' (403) - Check Permissions"
            else:
                msg = f"S3 ClientError: {e}"
            
            logger.error(f"‚ùå {msg}")
            self.init_error = msg
            self.enabled = False
            
        except BotoCoreError as e:
            msg = f"BotoCoreError: {e}"
            logger.error(f"‚ùå {msg}", exc_info=True)
            self.init_error = msg
            self.enabled = False
        except Exception as e:
            msg = f"Unexpected Init Error: {e}"
            logger.error(f"‚ùå {msg}", exc_info=True)
            self.init_error = msg
            self.enabled = False
    
    
    def upload_file(self, file_content: Union[bytes, BinaryIO], key: str, 
                   content_type: str = 'application/octet-stream') -> Optional[str]:
        """
        UploadeazƒÉ un fi»ôier √Æn S3.
        
        Args:
            file_content: Con»õinutul fi»ôierului (bytes sau file-like object)
            key: Calea √Æn bucket (ex: "abc123/csvs/file.csv")
            content_type: MIME type (ex: "text/csv", "application/pdf", "image/png")
            
        Returns:
            str: URL-ul fi»ôierului uploadat sau None dacƒÉ eroare
        """
        if not self.enabled:
            reason = self.init_error if self.init_error else "Unknown Reason"
            logger.warning(f"‚ö†Ô∏è S3 disabled (Reason: {reason}) - file {key} NOT uploaded to cloud")
            return self._save_local_fallback(file_content, key)
        
        try:
            # Convertim la bytes dacƒÉ e file-like object
            if hasattr(file_content, 'read'):
                file_content = file_content.read()
            
            file_size_bytes = len(file_content)
            file_size_mb = file_size_bytes / (1024 * 1024)
            
            # [ITERATION 2] Add timestamp for upload tracking
            import time
            upload_start = time.time()
            logger.warning(f"üöÄ [S3_TRACE_UPLOAD] START Upload: {key} | Size: {file_size_mb:.2f} MB ({file_size_bytes} bytes) | Time: {upload_start}")
            
            # Upload cƒÉtre S3
            self.client.put_object(
                Bucket=self.bucket_name,
                Key=key,
                Body=file_content,
                ContentType=content_type
            )
            
            # [ITERATION 2] Log upload duration
            upload_duration = time.time() - upload_start
            logger.warning(f"‚úÖ [S3_TRACE_UPLOAD] SUCCESS Upload: {key} | Size: {file_size_mb:.2f} MB | Duration: {upload_duration:.2f}s")
            
            # ReturnƒÉm URL-ul (format: https://bucket.endpoint/key sau endpoint/bucket/key)
            # Adaptare pentru endpoint-uri care nu au bucket-ul √Æn subdomain
            if self.bucket_name in S3_ENDPOINT:
                 url = f"{S3_ENDPOINT}/{key}"
            else:
                 url = f"{S3_ENDPOINT}/{self.bucket_name}/{key}"

            return url
            
        except ClientError as e:
            # [DIAGNOSTIC] Granular Error Logging
            error_code = e.response.get('Error', {}).get('Code', 'Unknown')
            request_id = e.response.get('ResponseMetadata', {}).get('RequestId', 'Unknown')
            http_status = e.response.get('ResponseMetadata', {}).get('HTTPStatusCode', 'Unknown')
            
            logger.error(f"‚ùå [S3_TRACE_UPLOAD] FAIL Upload S3 pentru {key}")
            logger.error(f"   - Error Code: {error_code}")
            logger.error(f"   - HTTP Status: {http_status}")
            logger.error(f"   - Request ID: {request_id}")
            logger.error(f"   - Full Exception: {e}")
            
            # [ITERATION 2] Log the exact moment of fallback
            logger.warning(f"üîÑ [S3_FALLBACK] Switching to LOCAL storage for {key} due to S3 error")
            logger.warning(f"üîÑ [S3_FALLBACK] Reason: {error_code} (HTTP {http_status})")
            
            # [ITERATION 4] Detect specific error scenarios
            if error_code == 'QuotaExceeded' or 'quota' in str(e).lower():
                logger.critical(f"üíæ [S3_QUOTA] BUCKET QUOTA EXCEEDED! Check Scaleway Object Storage limits.")
            elif error_code == 'AccessDenied' or error_code == '403':
                logger.critical(f"üîí [S3_PERMISSION] Token lacks WRITE permission. Check API Token scopes.")
            elif 'cors' in str(e).lower():
                logger.warning(f"üåê [S3_CORS] Possible CORS policy issue.")
            
            # Fallback: salvƒÉm local
            return self._save_local_fallback(file_content, key)
    
    
    def download_file(self, key: str) -> Optional[bytes]:
        """
        DescarcƒÉ un fi»ôier din S3.
        
        Args:
            key: Calea √Æn bucket (ex: "abc123/csvs/file.csv")
            
        Returns:
            bytes: Con»õinutul fi»ôierului sau None dacƒÉ eroare
        """
        if not self.enabled:
            logger.warning(f"‚ö†Ô∏è S3 dezactivat - √Æncercare download local pentru {key}")
            return self._read_local_fallback(key)
        
        try:
            logger.warning(f"üîΩ [S3_TRACE_DOWNLOAD] START Download: {key}")
            response = self.client.get_object(
                Bucket=self.bucket_name,
                Key=key
            )
            
            file_content = response['Body'].read()
            file_size_mb = len(file_content) / (1024 * 1024)
            logger.warning(f"‚úÖ [S3_TRACE_DOWNLOAD] SUCCESS Download: {key} | Size: {file_size_mb:.2f} MB")
            
            return file_content
            
        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', 'Unknown')
            if error_code == 'NoSuchKey':
                logger.warning(f"‚ö†Ô∏è [S3_TRACE_DOWNLOAD] NoSuchKey - Fi»ôierul nu existƒÉ √Æn S3: {key}")
            else:
                logger.error(f"‚ùå [S3_TRACE_DOWNLOAD] FAIL Download S3 pentru {key}: {e}", exc_info=True)
            
            # Fallback: citim local
            return self._read_local_fallback(key)
    
    
    def delete_file(self, key: str) -> bool:
        """
        »òterge un fi»ôier din S3.
        
        Args:
            key: Calea √Æn bucket (ex: "abc123/csvs/file.csv")
            
        Returns:
            bool: True dacƒÉ »ôters cu succes, False altfel
        """
        if not self.enabled:
            logger.warning(f"‚ö†Ô∏è S3 dezactivat - »ôtergere localƒÉ pentru {key}")
            return self._delete_local_fallback(key)
        
        try:
            logger.warning(f"üóëÔ∏è [S3_TRACE_DELETE] Attempt delete: {key}")
            self.client.delete_object(
                Bucket=self.bucket_name,
                Key=key
            )
            logger.warning(f"‚úÖ [S3_TRACE_DELETE] SUCCESS Delete: {key}")
            return True
            
        except ClientError as e:
            logger.error(f"‚ùå Eroare »ôtergere S3 pentru {key}: {e}", exc_info=True)
            return False
    
    
    def list_files(self, prefix: str = "") -> list[str]:
        """
        ListeazƒÉ fi»ôierele dintr-un folder S3.
        
        Args:
            prefix: Prefixul cƒÉii (ex: "abc123/csvs/")
            
        Returns:
            list: Lista de chei (cƒÉi) ale fi»ôierelor
        """
        if not self.enabled:
            logger.warning(f"‚ö†Ô∏è S3 dezactivat - listare localƒÉ pentru {prefix}")
            return self._list_local_fallback(prefix)
        
        try:
            response = self.client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=prefix
            )
            
            files = [obj['Key'] for obj in response.get('Contents', [])]
            logger.info(f"üìÇ GƒÉsite {len(files)} fi»ôiere √Æn S3 cu prefix '{prefix}'")
            return files
            
        except ClientError as e:
            logger.error(f"‚ùå Eroare listare S3 pentru {prefix}: {e}", exc_info=True)
            return []
    
    
    def generate_presigned_url(self, key: str, expiration: int = 3600) -> Optional[str]:
        """
        GenereazƒÉ un URL cu semnƒÉturƒÉ temporarƒÉ pentru acces direct (op»õional).
        
        Args:
            key: Calea √Æn bucket
            expiration: Timp de expirare √Æn secunde (default: 1 orƒÉ)
            
        Returns:
            str: URL signed sau None
        """
        if not self.enabled:
            logger.warning(f"‚ö†Ô∏è S3 dezactivat - nu se poate genera URL signed pentru {key}")
            return None
        
        try:
            url = self.client.generate_presigned_url(
                'get_object',
                Params={'Bucket': self.bucket_name, 'Key': key},
                ExpiresIn=expiration
            )
            logger.info(f"üîó URL signed generat pentru {key} (expirƒÉ √Æn {expiration}s)")
            return url
            
        except ClientError as e:
            logger.error(f"‚ùå Eroare generare URL signed pentru {key}: {e}", exc_info=True)
            return None
    
    
    # ==============================================================================
    # FALLBACK - STOCARE LOCALƒÇ (dacƒÉ S3 nu e disponibil)
    # ==============================================================================
    
    def _save_local_fallback(self, content: bytes, key: str) -> Optional[str]:
        """SalveazƒÉ fi»ôierul local ca fallback."""
        try:
            local_path = os.path.join(LOCAL_STORAGE_DIR, key)
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            with open(local_path, 'wb') as f:
                f.write(content)
            
            logger.info(f"üíæ Fi»ôier salvat LOCAL (fallback): {local_path}")
            return local_path
            
        except Exception as e:
            logger.error(f"‚ùå Eroare salvare localƒÉ pentru {key}: {e}", exc_info=True)
            return None
    
    
    def _read_local_fallback(self, key: str) -> Optional[bytes]:
        """Cite»ôte fi»ôierul local ca fallback."""
        try:
            local_path = os.path.join(LOCAL_STORAGE_DIR, key)
            
            if not os.path.exists(local_path):
                logger.warning(f"‚ö†Ô∏è Fi»ôier inexistent local: {local_path}")
                return None
            
            with open(local_path, 'rb') as f:
                content = f.read()
            
            logger.info(f"üìÇ Fi»ôier citit LOCAL (fallback): {local_path}")
            return content
            
        except Exception as e:
            logger.error(f"‚ùå Eroare citire localƒÉ pentru {key}: {e}", exc_info=True)
            return None
    
    
    def _delete_local_fallback(self, key: str) -> bool:
        """»òterge fi»ôierul local ca fallback."""
        try:
            local_path = os.path.join(LOCAL_STORAGE_DIR, key)
            
            if os.path.exists(local_path):
                os.remove(local_path)
                logger.info(f"üóëÔ∏è Fi»ôier »ôters LOCAL (fallback): {local_path}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è Fi»ôier inexistent local pentru »ôtergere: {local_path}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Eroare »ôtergere localƒÉ pentru {key}: {e}", exc_info=True)
            return False
    
    
    def _list_local_fallback(self, prefix: str) -> list[str]:
        """ListeazƒÉ fi»ôierele locale ca fallback."""
        try:
            local_path = os.path.join(LOCAL_STORAGE_DIR, prefix)
            
            if not os.path.exists(local_path):
                return []
            
            files = []
            for root, _, filenames in os.walk(local_path):
                for filename in filenames:
                    full_path = os.path.join(root, filename)
                    # Convertim calea √Æn format key (relativ la LOCAL_STORAGE_DIR)
                    key = os.path.relpath(full_path, LOCAL_STORAGE_DIR).replace('\\', '/')
                    files.append(key)
            
            logger.info(f"üìÇ GƒÉsite {len(files)} fi»ôiere LOCAL (fallback) cu prefix '{prefix}'")
            return files
            
        except Exception as e:
            logger.error(f"‚ùå Eroare listare localƒÉ pentru {prefix}: {e}", exc_info=True)
            return []

    def _check_write_permission(self):
        """
        [DIAGNOSTIC] TesteazƒÉ explicit permisiunea de scriere (PUT).
        Unele token-uri Scaleway au doar 'Object Read' dar nu 'Object Write'.
        """
        try:
            test_key = "diagnostic_write_check.txt"
            logger.warning(f"üïµÔ∏è [S3_PERM_CHECK] Testing WRITE permission on {test_key}...")
            
            self.client.put_object(
                Bucket=self.bucket_name,
                Key=test_key,
                Body=b"write_test",
                ContentType="text/plain"
            )
            logger.warning(f"‚úÖ [S3_PERM_CHECK] WRITE Permission CONFIRMED!")
            
            # Cleanup
            self.client.delete_object(Bucket=self.bucket_name, Key=test_key)
            
        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code', 'Unknown')
            logger.critical(f"‚ùå [S3_PERM_CHECK] WRITE Permission FAILED! Code: {error_code}")
            logger.critical(f"   - Sfat: VerificƒÉ Token Permissions √Æn Scaleway Console")
            logger.critical(f"   - Token trebuie sƒÉ aibƒÉ: ObjectStorageReadOnly=false + ObjectStorageReadWrite=true")
            logger.critical(f"   - Location: Scaleway Console ‚Üí Identity and Access Management (IAM) ‚Üí API Keys")
        except Exception as e:
            logger.critical(f"‚ùå [S3_PERM_CHECK] Write Check Failed Unexpectedly: {e}")


# ==============================================================================
# INSTAN»öƒÇ GLOBALƒÇ - SINGLETON
# ==============================================================================

# CreƒÉm o instan»õƒÉ globalƒÉ pentru a fi folositƒÉ √Æn toatƒÉ aplica»õia
# Variabila pƒÉstratƒÉ ca r2_client pentru backward compatibility (op»õional putem schimba √Æn s3_client)
# Vom alinia totul la s3_client intern
s3_client = S3StorageClient()
r2_client = s3_client # Alias pentru cod vechi


# ==============================================================================
# FUNC»öII HELPER - INTERFA»öƒÇ SIMPLIFICATƒÇ
# ==============================================================================

def upload_patient_csv(token: str, csv_content: bytes, filename: str) -> Optional[str]:
    """
    UploadeazƒÉ CSV pacient √Æn S3.
    
    Args:
        token: UUID pacient
        csv_content: Con»õinutul CSV (bytes)
        filename: Numele fi»ôierului original
        
    Returns:
        str: URL sau calea fi»ôierului
    """
    key = f"{token}/csvs/{filename}"
    return s3_client.upload_file(csv_content, key, content_type='text/csv')


def upload_patient_pdf(token: str, pdf_content: bytes, filename: str) -> Optional[str]:
    """
    UploadeazƒÉ PDF raport pacient √Æn S3.
    
    Args:
        token: UUID pacient
        pdf_content: Con»õinutul PDF (bytes)
        filename: Numele fi»ôierului original
        
    Returns:
        str: URL sau calea fi»ôierului
    """
    key = f"{token}/pdfs/{filename}"
    return s3_client.upload_file(pdf_content, key, content_type='application/pdf')


def upload_patient_plot(token: str, plot_content: bytes, filename: str) -> Optional[str]:
    """
    UploadeazƒÉ grafic PNG pacient √Æn S3.
    
    Args:
        token: UUID pacient
        plot_content: Con»õinutul PNG (bytes)
        filename: Numele fi»ôierului original
        
    Returns:
        str: URL sau calea fi»ôierului
    """
    key = f"{token}/plots/{filename}"
    return s3_client.upload_file(plot_content, key, content_type='image/png')


def download_patient_file(token: str, file_type: str, filename: str) -> Optional[bytes]:
    """
    DescarcƒÉ un fi»ôier pacient din S3.
    
    Args:
        token: UUID pacient
        file_type: Tipul fi»ôierului ('csvs', 'pdfs', 'plots')
        filename: Numele fi»ôierului
        
    Returns:
        bytes: Con»õinutul fi»ôierului sau None
    """
    key = f"{token}/{file_type}/{filename}"
    return s3_client.download_file(key)


def list_patient_files(token: str, file_type: str = "") -> list[str]:
    """
    ListeazƒÉ fi»ôierele unui pacient.
    
    Args:
        token: UUID pacient
        file_type: Tipul fi»ôierelor ('csvs', 'pdfs', 'plots') sau '' pentru toate
        
    Returns:
        list: Lista de fi»ôiere
    """
    prefix = f"{token}/{file_type}" if file_type else f"{token}/"
    return s3_client.list_files(prefix)


def delete_patient_folder(token: str) -> bool:
    """
    »òterge TOATE fi»ôierele unui pacient (DANGER ZONE!).
    
    Args:
        token: UUID pacient
        
    Returns:
        bool: True dacƒÉ »ôters cu succes
    """
    try:
        files = list_patient_files(token)
        
        for file_key in files:
            s3_client.delete_file(file_key)
        
        logger.info(f"üóëÔ∏è Folder pacient {token[:8]}... »ôters complet ({len(files)} fi»ôiere)")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Eroare »ôtergere folder pacient {token[:8]}...: {e}", exc_info=True)
        return False


# ==============================================================================
# STATUS CHECK - DEBUGGING
# ==============================================================================

def get_storage_status() -> dict:
    """
    ReturneazƒÉ statusul storage-ului (S3 sau local).
    
    Returns:
        dict: Informa»õii despre storage
    """
    return {
        "s3_enabled": s3_client.enabled,
        "s3_endpoint": S3_ENDPOINT if s3_client.enabled else "N/A",
        "s3_bucket": S3_BUCKET_NAME if s3_client.enabled else "N/A",
        "fallback_storage": LOCAL_STORAGE_DIR,
        "mode": "S3 Storage Cloud" if s3_client.enabled else "Local Storage (Fallback)"
    }


if __name__ == "__main__":
    # Test rapid pentru verificare configurare
    logger.info("=== TEST S3 STORAGE (Generic) ===")
    status = get_storage_status()
    
    for key, value in status.items():
        logger.info(f"  {key}: {value}")
    
    if s3_client.enabled:
        logger.info("‚úÖ S3 Storage este ACTIV »ôi func»õional!")
    else:
        logger.warning("‚ö†Ô∏è S3 Storage este DEZACTIVAT - folosim stocare localƒÉ")
